{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment #5: Extraction of subject–verb–object triples\n",
    "Author: Pierre Nugues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this assignment, you will extract relations from a parsed sentence involving two words or entities. You will start with pairs of words, namely a subject and its verb, and then extend your programs to triples: subject, verb, and object. In the triples, the subject and the object are the entities, and the verb represents the relation. \n",
    "\n",
    "$$\n",
    "\\text{Subject} \\xrightarrow[\\text{}]{\\text{Verb}} \\text{Object}\n",
    "$$\n",
    "\n",
    "The overall work is inspired by the _Prismatic_ knowledge base used in the IBM Watson system, where the subject, verb, and object triples are a way to extract knowledge from text.  See <a href=\"http://www.aclweb.org/anthology/W/W10/W10-0915.pdf\">this paper</a> for details. \n",
    "\n",
    "You will apply the extraction to multilingual texts: \n",
    "1. First you will use a parsed corpus of Swedish; and then\n",
    "2. You will apply it to other languages.\n",
    "            \n",
    "The objectives of this assignment are to:\n",
    "* Extract the subject–verb pairs from a parsed corpus\n",
    "* Extend the extraction to subject–verb–object triples\n",
    "* Understand how dependency parsing can help create a knowledge base\n",
    "* Write a short report of 1 to 2 pages on the assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As corpora, you will use the Universal Dependencies: https://universaldependencies.org/.\n",
    "1. In the first part of the assignment, you will focus on Swedish as it is easier to understand for most students, and then \n",
    "2. Move on to all the other languages. \n",
    "\n",
    "You will only consider the training sets of each corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choosing a parsed corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "1. Download manually the latest version of Universal dependencies (2.8.1); see here `https://lindat.mff.cuni.cz/repository/xmlui/handle/11234/1-3687`;\n",
    "2. Alternatively, you can run the command below, preferably from a terminal window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "  9  410M    9 37.1M    0     0  30.8M      0  0:00:13  0:00:01  0:00:12 30.7M\n",
      " 21  410M   21 89.5M    0     0  40.6M      0  0:00:10  0:00:02  0:00:08 40.6M\n",
      " 39  410M   39  160M    0     0  50.0M      0  0:00:08  0:00:03  0:00:05 50.0M\n",
      " 56  410M   56  231M    0     0  55.0M      0  0:00:07  0:00:04  0:00:03 55.0M\n",
      " 60  410M   60  248M    0     0  47.5M      0  0:00:08  0:00:05  0:00:03 49.5M\n",
      " 67  410M   67  276M    0     0  44.5M      0  0:00:09  0:00:06  0:00:03 47.8M\n",
      " 73  410M   73  302M    0     0  41.5M      0  0:00:09  0:00:07  0:00:02 42.0M\n",
      " 79  410M   79  327M    0     0  39.5M      0  0:00:10  0:00:08  0:00:02 32.9M\n",
      " 85  410M   85  350M    0     0  37.9M      0  0:00:10  0:00:09  0:00:01 23.6M\n",
      " 89  410M   89  366M    0     0  35.8M      0  0:00:11  0:00:10  0:00:01 23.6M\n",
      " 97  410M   97  400M    0     0  35.6M      0  0:00:11  0:00:11 --:--:-- 24.7M\n",
      "100  410M  100  410M    0     0  35.3M      0  0:00:11  0:00:11 --:--:-- 24.8M\n"
     ]
    }
   ],
   "source": [
    "!curl \"https://lindat.mff.cuni.cz/repository/xmlui/bitstream/handle/11234/1-3687/ud-treebanks-v2.8.tgz?sequence=1&isAllowed=y\" --output my_ud_file.tgz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uncompress the archive and:\n",
    "1. Go to the Swedish _Talbanken_ corpus;\n",
    "2. Read the CoNLL-U annotation here: https://universaldependencies.org/format.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Examining the annotation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will carry out the following steps and describe them in your report:\n",
    "\n",
    "1. Draw graphical representations of the two first Swedish sentences of the training set. You will include these drawings in your report;\n",
    "2. Visualize these sentences with this tool: http://spyysalo.github.io/conllu.js/ and check that you have the same results;\n",
    "3. Apply the dependency parser for Swedish of the <a href=\"http://vilde.cs.lth.se:9000/\">Langforia pipelines</a> to these sentences (only the text of each sentence). You will have to select Swedish and activate both `Token` and `DependencyRelation`. Link to Lanforia pipelines: <a href=\"http://vilde.cs.lth.se:9000/\">http://vilde.cs.lth.se:9000/</a>. You will describe possible differences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Programming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Swedish"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will extract all the subject–verb pairs and the subject–verb–object triples from the Swedish _Talbanken_ training corpus. To start the program, you can use the CoNLL-U reader available in the cells below.\n",
    "This program works for the other corpora. You can also program a reader yourself starting from the one you used to read the CoNLL 2000 format in the fourth lab or from scratch. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Corpus location"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the corpus locations you will use. You may have to adjust `ud_path`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "ud_path = 'C:/Users/4stra/Documents/pyton/FDAN20/env/edan20/notebooks/ud-treebanks-v2.8/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_sv = ud_path + 'UD_Swedish-Talbanken/sv_talbanken-ud-train.conllu'\n",
    "path_fr = ud_path + 'UD_French-GSD/fr_gsd-ud-train.conllu'\n",
    "path_ru = ud_path + 'UD_Russian-SynTagRus/ru_syntagrus-ud-train.conllu'\n",
    "path_en = ud_path + 'UD_English-EWT/en_ewt-ud-train.conllu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The column names of the CoNLL-U corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_names_u = ['ID', 'FORM', 'LEMMA', 'UPOS', 'XPOS', 'FEATS', 'HEAD', 'DEPREL', 'DEPS', 'MISC']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Functions to read the CoNLL-U files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_sentences(file):\n",
    "    \"\"\"\n",
    "    Creates a list of sentences from the corpus\n",
    "    Each sentence is a string\n",
    "    :param file:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    f = open(file, encoding='utf-8').read().strip()\n",
    "    sentences = f.split('\\n\\n')\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_rows(sentences, column_names):\n",
    "    \"\"\"\n",
    "    Creates a list of sentence where each sentence is a list of lines\n",
    "    Each line is a dictionary of columns\n",
    "    :param sentences:\n",
    "    :param column_names:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    new_sentences = []\n",
    "    root_values = ['0', 'ROOT', 'ROOT', 'ROOT', 'ROOT', 'ROOT', '0', 'ROOT', '0', 'ROOT']\n",
    "    start = [dict(zip(column_names, root_values))]\n",
    "    for sentence in sentences:\n",
    "        rows = sentence.split('\\n')\n",
    "        sentence = [dict(zip(column_names, row.split('\\t'))) for row in rows if row[0] != '#']\n",
    "        sentence = start + sentence\n",
    "        new_sentences.append(sentence)\n",
    "    return new_sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Reading the corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load the Swedish _Talbanken_ corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = read_sentences(path_sv)\n",
    "formatted_corpus = split_rows(sentences, column_names_u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = read_sentences(path_sv)\n",
    "formatted_corpus = split_rows(sentences, column_names_u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4303"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(formatted_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The parsed sentence: _Genom skattereformen införs individuell beskattning (särbeskattning) av arbetsinkomster._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'ID': '0',\n",
       "  'FORM': 'ROOT',\n",
       "  'LEMMA': 'ROOT',\n",
       "  'UPOS': 'ROOT',\n",
       "  'XPOS': 'ROOT',\n",
       "  'FEATS': 'ROOT',\n",
       "  'HEAD': '0',\n",
       "  'DEPREL': 'ROOT',\n",
       "  'DEPS': '0',\n",
       "  'MISC': 'ROOT'},\n",
       " {'ID': '1',\n",
       "  'FORM': 'Genom',\n",
       "  'LEMMA': 'genom',\n",
       "  'UPOS': 'ADP',\n",
       "  'XPOS': 'PP',\n",
       "  'FEATS': '_',\n",
       "  'HEAD': '2',\n",
       "  'DEPREL': 'case',\n",
       "  'DEPS': '2:case',\n",
       "  'MISC': '_'},\n",
       " {'ID': '2',\n",
       "  'FORM': 'skattereformen',\n",
       "  'LEMMA': 'skattereform',\n",
       "  'UPOS': 'NOUN',\n",
       "  'XPOS': 'NN|UTR|SIN|DEF|NOM',\n",
       "  'FEATS': 'Case=Nom|Definite=Def|Gender=Com|Number=Sing',\n",
       "  'HEAD': '3',\n",
       "  'DEPREL': 'obl',\n",
       "  'DEPS': '3:obl:genom',\n",
       "  'MISC': '_'},\n",
       " {'ID': '3',\n",
       "  'FORM': 'införs',\n",
       "  'LEMMA': 'införa',\n",
       "  'UPOS': 'VERB',\n",
       "  'XPOS': 'VB|PRS|SFO',\n",
       "  'FEATS': 'Mood=Ind|Tense=Pres|VerbForm=Fin|Voice=Pass',\n",
       "  'HEAD': '0',\n",
       "  'DEPREL': 'root',\n",
       "  'DEPS': '0:root',\n",
       "  'MISC': '_'},\n",
       " {'ID': '4',\n",
       "  'FORM': 'individuell',\n",
       "  'LEMMA': 'individuell',\n",
       "  'UPOS': 'ADJ',\n",
       "  'XPOS': 'JJ|POS|UTR|SIN|IND|NOM',\n",
       "  'FEATS': 'Case=Nom|Definite=Ind|Degree=Pos|Gender=Com|Number=Sing',\n",
       "  'HEAD': '5',\n",
       "  'DEPREL': 'amod',\n",
       "  'DEPS': '5:amod',\n",
       "  'MISC': '_'},\n",
       " {'ID': '5',\n",
       "  'FORM': 'beskattning',\n",
       "  'LEMMA': 'beskattning',\n",
       "  'UPOS': 'NOUN',\n",
       "  'XPOS': 'NN|UTR|SIN|IND|NOM',\n",
       "  'FEATS': 'Case=Nom|Definite=Ind|Gender=Com|Number=Sing',\n",
       "  'HEAD': '3',\n",
       "  'DEPREL': 'nsubj:pass',\n",
       "  'DEPS': '3:nsubj:pass',\n",
       "  'MISC': '_'},\n",
       " {'ID': '6',\n",
       "  'FORM': '(',\n",
       "  'LEMMA': '(',\n",
       "  'UPOS': 'PUNCT',\n",
       "  'XPOS': 'PAD',\n",
       "  'FEATS': '_',\n",
       "  'HEAD': '5',\n",
       "  'DEPREL': 'punct',\n",
       "  'DEPS': '5:punct',\n",
       "  'MISC': 'SpaceAfter=No'},\n",
       " {'ID': '7',\n",
       "  'FORM': 'särbeskattning',\n",
       "  'LEMMA': 'särbeskattning',\n",
       "  'UPOS': 'NOUN',\n",
       "  'XPOS': 'NN|UTR|SIN|IND|NOM',\n",
       "  'FEATS': 'Case=Nom|Definite=Ind|Gender=Com|Number=Sing',\n",
       "  'HEAD': '5',\n",
       "  'DEPREL': 'appos',\n",
       "  'DEPS': '5:appos',\n",
       "  'MISC': 'SpaceAfter=No'},\n",
       " {'ID': '8',\n",
       "  'FORM': ')',\n",
       "  'LEMMA': ')',\n",
       "  'UPOS': 'PUNCT',\n",
       "  'XPOS': 'PAD',\n",
       "  'FEATS': '_',\n",
       "  'HEAD': '5',\n",
       "  'DEPREL': 'punct',\n",
       "  'DEPS': '5:punct',\n",
       "  'MISC': '_'},\n",
       " {'ID': '9',\n",
       "  'FORM': 'av',\n",
       "  'LEMMA': 'av',\n",
       "  'UPOS': 'ADP',\n",
       "  'XPOS': 'PP',\n",
       "  'FEATS': '_',\n",
       "  'HEAD': '10',\n",
       "  'DEPREL': 'case',\n",
       "  'DEPS': '10:case',\n",
       "  'MISC': '_'},\n",
       " {'ID': '10',\n",
       "  'FORM': 'arbetsinkomster',\n",
       "  'LEMMA': 'arbetsinkomst',\n",
       "  'UPOS': 'NOUN',\n",
       "  'XPOS': 'NN|UTR|PLU|IND|NOM',\n",
       "  'FEATS': 'Case=Nom|Definite=Ind|Gender=Com|Number=Plur',\n",
       "  'HEAD': '5',\n",
       "  'DEPREL': 'nmod',\n",
       "  'DEPS': '5:nmod:av',\n",
       "  'MISC': 'SpaceAfter=No'},\n",
       " {'ID': '11',\n",
       "  'FORM': '.',\n",
       "  'LEMMA': '.',\n",
       "  'UPOS': 'PUNCT',\n",
       "  'XPOS': 'MAD',\n",
       "  'FEATS': '_',\n",
       "  'HEAD': '3',\n",
       "  'DEPREL': 'punct',\n",
       "  'DEPS': '3:punct',\n",
       "  'MISC': '_'}]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "formatted_corpus[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Converting the lists in dictionaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To ease the processing of some corpora, you will use a dictionary represention of the sentences. The keys will be the `ID` values. We do this because `ID` is not necessarily a number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_dict(formatted_corpus):\n",
    "    \"\"\"\n",
    "    Converts each sentence from a list of words to a dictionary where the keys are id\n",
    "    :param formatted_corpus:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    formatted_corpus_dict = []\n",
    "    for sentence in formatted_corpus:\n",
    "        sentence_dict = {}\n",
    "        for word in sentence:\n",
    "            sentence_dict[word['ID']] = word\n",
    "        formatted_corpus_dict.append(sentence_dict)\n",
    "    return formatted_corpus_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'0': {'ID': '0',\n",
       "  'FORM': 'ROOT',\n",
       "  'LEMMA': 'ROOT',\n",
       "  'UPOS': 'ROOT',\n",
       "  'XPOS': 'ROOT',\n",
       "  'FEATS': 'ROOT',\n",
       "  'HEAD': '0',\n",
       "  'DEPREL': 'ROOT',\n",
       "  'DEPS': '0',\n",
       "  'MISC': 'ROOT'},\n",
       " '1': {'ID': '1',\n",
       "  'FORM': 'Genom',\n",
       "  'LEMMA': 'genom',\n",
       "  'UPOS': 'ADP',\n",
       "  'XPOS': 'PP',\n",
       "  'FEATS': '_',\n",
       "  'HEAD': '2',\n",
       "  'DEPREL': 'case',\n",
       "  'DEPS': '2:case',\n",
       "  'MISC': '_'},\n",
       " '2': {'ID': '2',\n",
       "  'FORM': 'skattereformen',\n",
       "  'LEMMA': 'skattereform',\n",
       "  'UPOS': 'NOUN',\n",
       "  'XPOS': 'NN|UTR|SIN|DEF|NOM',\n",
       "  'FEATS': 'Case=Nom|Definite=Def|Gender=Com|Number=Sing',\n",
       "  'HEAD': '3',\n",
       "  'DEPREL': 'obl',\n",
       "  'DEPS': '3:obl:genom',\n",
       "  'MISC': '_'},\n",
       " '3': {'ID': '3',\n",
       "  'FORM': 'införs',\n",
       "  'LEMMA': 'införa',\n",
       "  'UPOS': 'VERB',\n",
       "  'XPOS': 'VB|PRS|SFO',\n",
       "  'FEATS': 'Mood=Ind|Tense=Pres|VerbForm=Fin|Voice=Pass',\n",
       "  'HEAD': '0',\n",
       "  'DEPREL': 'root',\n",
       "  'DEPS': '0:root',\n",
       "  'MISC': '_'},\n",
       " '4': {'ID': '4',\n",
       "  'FORM': 'individuell',\n",
       "  'LEMMA': 'individuell',\n",
       "  'UPOS': 'ADJ',\n",
       "  'XPOS': 'JJ|POS|UTR|SIN|IND|NOM',\n",
       "  'FEATS': 'Case=Nom|Definite=Ind|Degree=Pos|Gender=Com|Number=Sing',\n",
       "  'HEAD': '5',\n",
       "  'DEPREL': 'amod',\n",
       "  'DEPS': '5:amod',\n",
       "  'MISC': '_'},\n",
       " '5': {'ID': '5',\n",
       "  'FORM': 'beskattning',\n",
       "  'LEMMA': 'beskattning',\n",
       "  'UPOS': 'NOUN',\n",
       "  'XPOS': 'NN|UTR|SIN|IND|NOM',\n",
       "  'FEATS': 'Case=Nom|Definite=Ind|Gender=Com|Number=Sing',\n",
       "  'HEAD': '3',\n",
       "  'DEPREL': 'nsubj:pass',\n",
       "  'DEPS': '3:nsubj:pass',\n",
       "  'MISC': '_'},\n",
       " '6': {'ID': '6',\n",
       "  'FORM': '(',\n",
       "  'LEMMA': '(',\n",
       "  'UPOS': 'PUNCT',\n",
       "  'XPOS': 'PAD',\n",
       "  'FEATS': '_',\n",
       "  'HEAD': '5',\n",
       "  'DEPREL': 'punct',\n",
       "  'DEPS': '5:punct',\n",
       "  'MISC': 'SpaceAfter=No'},\n",
       " '7': {'ID': '7',\n",
       "  'FORM': 'särbeskattning',\n",
       "  'LEMMA': 'särbeskattning',\n",
       "  'UPOS': 'NOUN',\n",
       "  'XPOS': 'NN|UTR|SIN|IND|NOM',\n",
       "  'FEATS': 'Case=Nom|Definite=Ind|Gender=Com|Number=Sing',\n",
       "  'HEAD': '5',\n",
       "  'DEPREL': 'appos',\n",
       "  'DEPS': '5:appos',\n",
       "  'MISC': 'SpaceAfter=No'},\n",
       " '8': {'ID': '8',\n",
       "  'FORM': ')',\n",
       "  'LEMMA': ')',\n",
       "  'UPOS': 'PUNCT',\n",
       "  'XPOS': 'PAD',\n",
       "  'FEATS': '_',\n",
       "  'HEAD': '5',\n",
       "  'DEPREL': 'punct',\n",
       "  'DEPS': '5:punct',\n",
       "  'MISC': '_'},\n",
       " '9': {'ID': '9',\n",
       "  'FORM': 'av',\n",
       "  'LEMMA': 'av',\n",
       "  'UPOS': 'ADP',\n",
       "  'XPOS': 'PP',\n",
       "  'FEATS': '_',\n",
       "  'HEAD': '10',\n",
       "  'DEPREL': 'case',\n",
       "  'DEPS': '10:case',\n",
       "  'MISC': '_'},\n",
       " '10': {'ID': '10',\n",
       "  'FORM': 'arbetsinkomster',\n",
       "  'LEMMA': 'arbetsinkomst',\n",
       "  'UPOS': 'NOUN',\n",
       "  'XPOS': 'NN|UTR|PLU|IND|NOM',\n",
       "  'FEATS': 'Case=Nom|Definite=Ind|Gender=Com|Number=Plur',\n",
       "  'HEAD': '5',\n",
       "  'DEPREL': 'nmod',\n",
       "  'DEPS': '5:nmod:av',\n",
       "  'MISC': 'SpaceAfter=No'},\n",
       " '11': {'ID': '11',\n",
       "  'FORM': '.',\n",
       "  'LEMMA': '.',\n",
       "  'UPOS': 'PUNCT',\n",
       "  'XPOS': 'MAD',\n",
       "  'FEATS': '_',\n",
       "  'HEAD': '3',\n",
       "  'DEPREL': 'punct',\n",
       "  'DEPS': '3:punct',\n",
       "  'MISC': '_'}}"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "formatted_corpus_dict = convert_to_dict(formatted_corpus)\n",
    "formatted_corpus_dict[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting the subject-verb pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you will extract the subject-verb pairs, where you will set the words in lowercase. In the second sentence of the corpus, this corresponds to `(beskattning, införs)`. You will call the function `extract_pairs(formatted_corpus_dict)` and and you will store the results in a `pairs_sv` variable. All the corpora in the universal dependencies format use the same names for the grammatical functions. The subject and direct object are respectively `nsubj` and `obj`.\n",
    "\n",
    "You can use the algorithm you want. However, here are some hints on the results:\n",
    "* You will extract all the subject-verb pairs in the corpus. In the extraction, just check the function between two words. Do not check if the part of speech is a verb or a noun in the pair. You will also ignore the possible function suffixes as in `nsubj:pass`, where `pass` means passive.\n",
    "* You will return the results as Python's dictionaries, where the key will be the pair and the value, the count, as for instance `{(beskattning, införs): 1}`. Be sure you understand the Python dictionaries and note that you can use tuples as keys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_pairs(formatted_corpus_dict):\n",
    "    pairs_sv={}\n",
    "    for sen in formatted_corpus_dict:\n",
    "        for row in sen.keys():\n",
    "  \n",
    "            if sen[row]['DEPREL'].startswith('nsubj'):\n",
    "                HID=sen[row]['HEAD']\n",
    "                tup=(sen[row]['FORM'].lower(),sen[HID]['FORM'].lower())\n",
    " \n",
    "                if tup in pairs_sv.keys():\n",
    "                    pairs_sv[tup] +=1\n",
    "                else:\n",
    "                    pairs_sv[tup] =1\n",
    "    return pairs_sv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs_sv = extract_pairs(formatted_corpus_dict)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will compute the total number of subject-verb pairs. You should find 6,083 pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6083"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([pairs_sv[pair] for pair in pairs_sv])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finding the most frequent pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will sort your pairs by frequency and by lexical order of the pairs and store the five most frequent pairs in the `freq_pairs_sv` variable as in:\n",
    "```\n",
    "freq_pairs_sv = [(('som', 'har'), 45),\n",
    "\n",
    " ...]\n",
    " ````\n",
    "\n",
    "Here are the frequencies you should find:\n",
    "```\n",
    "45\n",
    "19\n",
    "19\n",
    "```\n",
    "When pairs have equal frequencies, you will sort them by alphabetical order, first term, and then second term of the pair."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In all the experiments, we will keep the `nbest` most frequent. In the first experiments, we set `nbest` to 3 first. We will set it to 5 in the last experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "nbest = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_pairs = sorted(pairs_sv, key=lambda x: (-pairs_sv[x], x))\n",
    "freq_pairs_sv = [(pair, pairs_sv[pair]) for pair in sorted_pairs][:nbest]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('som', 'har'), 45), (('du', 'får'), 19), (('vi', 'har'), 19)]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq_pairs_sv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting the subject-verb-object triples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will now extract all the subject–verb–object triples of the corpus. The object function uses the `obj` code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_triples(formatted_corpus_dict):\n",
    "    triples_sv={}\n",
    "    for sen in formatted_corpus_dict:\n",
    "        for row in sen.keys():\n",
    "            if sen[row]['DEPREL'].startswith('nsubj'):\n",
    "                HID=sen[row]['HEAD']\n",
    "                for rows in sen.keys():\n",
    "                    if sen[rows]['DEPREL'].startswith('obj'):\n",
    "                        HID2=sen[rows]['HEAD']\n",
    "                        if HID==HID2:\n",
    "                            tup=(sen[row]['FORM'].lower(),sen[HID]['FORM'].lower(),sen[rows]['FORM'].lower())\n",
    "                            if tup in triples_sv.keys():\n",
    "                                triples_sv[tup] +=1\n",
    "                            else:\n",
    "                                triples_sv[tup] =1\n",
    "    return triples_sv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "triples_sv = extract_triples(formatted_corpus_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the total number of triples. You should find 2054 triples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2054"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([triples_sv[triple] for triple in triples_sv])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finding the most frequent triples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will sort your triples by frequency and by lexical order of the pairs and store the three most frequent triples in the `freq_triples_sv` variable as in:\n",
    "```\n",
    "freq_triples_sv = [(('man', 'vänder', 'sig'), 14),\n",
    "\n",
    " ...]\n",
    " ````\n",
    "\n",
    "Here are the frequencies you should find:\n",
    "```\n",
    "14\n",
    "5\n",
    "3\n",
    "```\n",
    "As with the pairs of equal frequencies, you will sort the triples by alphabetical order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sorted_triples = sorted(triples_sv, key=lambda x: (-triples_sv[x], x))\n",
    "freq_triples_sv = [(triple, triples_sv[triple]) for triple in sorted_triples][:nbest]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('man', 'vänder', 'sig'), 14),\n",
       " (('det', 'rör', 'sig'), 5),\n",
       " (('man', 'söker', 'arbete'), 3)]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq_triples_sv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multilingual Corpora"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once your program is working on Swedish, you will apply it to all the other languages in universal dependencies. The code below returns all the files from a folder with a suffix. Here we consider the training files only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_files(dir, suffix):\n",
    "    \"\"\"\n",
    "    Returns all the files in a folder ending with suffix\n",
    "    Recursive version\n",
    "    :param dir:\n",
    "    :param suffix:\n",
    "    :return: the list of file names\n",
    "    \"\"\"\n",
    "    files = []\n",
    "    for file in os.listdir(dir):\n",
    "        path = dir + '/' + file\n",
    "        if os.path.isdir(path):\n",
    "            files += get_files(path, suffix)\n",
    "        elif os.path.isfile(path) and file.endswith(suffix):\n",
    "            files.append(path)\n",
    "    return files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = get_files(ud_path, 'train.conllu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dealing with the indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some corpora expand some tokens into multiwords. This is the case in French, Spanish, German, as well as English in some corpora.\n",
    "        The table below shows examples of such expansions.\n",
    "        <table style=\"width:100%\">\n",
    "            <tr>\n",
    "                <th>French</th>\n",
    "                <th>Spanish</th>\n",
    "                <th>German</th>\n",
    "            </tr>\n",
    "            <tr>\n",
    "                <td><i>du</i>: de le\n",
    "                </td>\n",
    "                <td><i>del</i>: de el\n",
    "                </td>\n",
    "                <td><i>zur</i>: zu der\n",
    "                </td>\n",
    "            </tr>\n",
    "            <tr>\n",
    "                <td><i>des</i>: de les\n",
    "                </td>\n",
    "                <td><i>vámonos</i>: vamos nos\n",
    "                </td>\n",
    "                <td><i>im</i>: in dem\n",
    "                </td>\n",
    "            </tr>\n",
    "        </table>\n",
    "        In the corpora, you have the original tokens as well as the multiwords as with <i>vámonos al mar</i> as in:\n",
    "        <pre>\n",
    "1-2 vámonos _\n",
    "1 vamos ir\n",
    "2 nos nosotros\n",
    "3-4 al _\n",
    "3 a a\n",
    "4 el el\n",
    "5 mar mar\n",
    "</pre>Read the format description for the details: [<a\n",
    "                href=\"http://universaldependencies.org/format.html\">CoNLL-U format</a>]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you represent the sentences as lists, the item indices are not reliable: In the format description,\n",
    "        the token at position 1 is <i>vamos</i> and not <i>vámonos</i>.\n",
    "        You have two ways to cope with this:\n",
    "1. Either remove all the lines that include a range in the `ID` field, or\n",
    "2. Encode the sentences as dictionaries (I felt this was preferable), where the keys are the `ID` numbers. This is what `convert_to_dict()` does. Here are the results for a sentence from the French CoNLL-U corpus:\n",
    "_Les iris du mâles sont jaunes toute l'année._ Note the `3-4` index and it expansion in `3`and `4`:\n",
    "```\n",
    "{'0': {'ID': '0',  'FORM': 'ROOT',  'LEMMA': 'ROOT',  'UPOS': 'ROOT',  'XPOS': 'ROOT',  'FEATS': 'ROOT',  'HEAD': '0',  'DEPREL': 'ROOT',  'DEPS': '0',  'MISC': 'ROOT'}, \n",
    "'1': {'ID': '1',  'FORM': 'Les',  'LEMMA': 'le',  'UPOS': 'DET',  'XPOS': '_',  'FEATS': 'Definite=Def|Gender=Masc|Number=Plur|PronType=Art',  'HEAD': '2',  'DEPREL': 'det',  'DEPS': '_',  'MISC': 'wordform=les'}, \n",
    "'2': {'ID': '2',  'FORM': 'iris',  'LEMMA': 'iris',  'UPOS': 'NOUN',  'XPOS': '_',  'FEATS': 'Gender=Masc|Number=Plur',  'HEAD': '7',  'DEPREL': 'nsubj',  'DEPS': '_',  'MISC': '_'}, \n",
    "'3-4': {'ID': '3-4',  'FORM': 'du',  'LEMMA': '_',  'UPOS': '_',  'XPOS': '_',  'FEATS': '_',  'HEAD': '_',  'DEPREL': '_',  'DEPS': '_',  'MISC': '_'}, \n",
    "'3': {'ID': '3',  'FORM': 'de',  'LEMMA': 'de',  'UPOS': 'ADP',  'XPOS': '_',  'FEATS': '_',  'HEAD': '5',  'DEPREL': 'case',  'DEPS': '_',  'MISC': '_'}, \n",
    "'4': {'ID': '4',  'FORM': 'le',  'LEMMA': 'le',  'UPOS': 'DET',  'XPOS': '_',  'FEATS': 'Definite=Def|Gender=Masc|Number=Sing|PronType=Art',  'HEAD': '5',  'DEPREL': 'det',  'DEPS': '_',  'MISC': '_'}, \n",
    "'5': {'ID': '5',  'FORM': 'mâles',  'LEMMA': 'mâle',  'UPOS': 'NOUN',  'XPOS': '_',  'FEATS': 'Gender=Masc|Number=Plur',  'HEAD': '2',  'DEPREL': 'nmod',  'DEPS': '_',  'MISC': '_'}, \n",
    "'6': {'ID': '6',  'FORM': 'sont',  'LEMMA': 'être',  'UPOS': 'AUX',  'XPOS': '_',  'FEATS': 'Mood=Ind|Number=Plur|Person=3|Tense=Pres|VerbForm=Fin',  'HEAD': '7',  'DEPREL': 'cop',  'DEPS': '_',  'MISC': '_'}, \n",
    "'7': {'ID': '7',  'FORM': 'jaunes',  'LEMMA': 'jaune',  'UPOS': 'ADJ',  'XPOS': '_',  'FEATS': 'Gender=Masc|Number=Plur',  'HEAD': '0',  'DEPREL': 'root',  'DEPS': '_',  'MISC': '_'}, \n",
    "'8': {'ID': '8',  'FORM': 'toute',  'LEMMA': 'tout',  'UPOS': 'ADJ',  'XPOS': '_',  'FEATS': 'Gender=Fem|Number=Sing',  'HEAD': '10',  'DEPREL': 'amod',  'DEPS': '_',  'MISC': '_'}, \n",
    "'9': {'ID': '9',  'FORM': \"l'\",  'LEMMA': 'le',  'UPOS': 'DET',  'XPOS': '_',  'FEATS': 'Definite=Def|Gender=Fem|Number=Sing|PronType=Art',  'HEAD': '10',  'DEPREL': 'det',  'DEPS': '_',  'MISC': 'SpaceAfter=No'}, \n",
    "'10': {'ID': '10',  'FORM': 'année',  'LEMMA': 'année',  'UPOS': 'NOUN',  'XPOS': '_',  'FEATS': 'Gender=Fem|Number=Sing',  'HEAD': '7',  'DEPREL': 'obl',  'DEPS': '_',  'MISC': 'SpaceAfter=No'}, \n",
    "'11': {'ID': '11',  'FORM': '.',  'LEMMA': '.',  'UPOS': 'PUNCT',  'XPOS': '_',  'FEATS': '_',  'HEAD': '7',  'DEPREL': 'punct',  'DEPS': '_',  'MISC': '_'}}\n",
    "```\n",
    "3. Some corpora have sentence numbers marked with a `#` symbol. You will ignore them and discard the lines starting with a `#`. This is already done in the CoNLL reader."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extracting the pairs and triples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a function `extract_pairs_and_triples(formatted_corpus_dict, nbest)` that extracts the `nbest` most frequent pairs and triples of a given corpus and returns two sorted lists of tuples: `frequent_pairs` and `frequent_triples`. You will sort them by frequency and then by alphabetical order of the pair or triple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_pairs_and_triples(formatted_corpus_dict, nbest):\n",
    "    triples=extract_triples(formatted_corpus_dict)\n",
    "    pairs=extract_pairs(formatted_corpus_dict)\n",
    "    sorted_triples = sorted(triples, key=lambda x: (-triples[x], x))\n",
    "    frequent_triples = [(triple, triples[triple]) for triple in sorted_triples][:nbest]      \n",
    "    #frequent_triples=sorted(frequent_triples,key=lambda x : x[0][0]+x[0][1]+x[0][2])\n",
    "    print(frequent_triples)\n",
    "    sorted_pairs = sorted(pairs, key=lambda x: (-pairs[x], x))\n",
    "    frequent_pairs = [(pair, pairs[pair]) for pair in sorted_pairs][:nbest]    \n",
    "    #frequent_pairs=sorted(frequent_pairs,key=lambda x : x[0][0]+x[0][1])\n",
    "    return frequent_pairs,frequent_triples\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run your extractor on all the corpora. Note that some corpora have replaced the words by underscores as for the FTB corpus in French. This is because the text is copyrighted. You need then to contact the provider and sign a license to obtain them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(('jy', 'doen', 'aansoek'), 14), (('maatskappy', 'doen', 'aansoek'), 6), (('wat', 'staar', 'ons'), 6)]\n",
      "[(('ἀχιλλεύς', 'προσέφη', 'τὸν'), 14), (('ζεύς', 'προσέφη', 'τὴν'), 10), (('ἕκτωρ', 'προσέφη', 'τὸν'), 10)]\n",
      "[(('ἡμεῖς', 'ἴδμεν', 'τῶν'), 11), (('πίστις', 'σέσωκέν', 'σε'), 6), (('πυθίη', 'χρᾷ', 'τάδε'), 6)]\n",
      "[(('_', '_', '_'), 18788)]\n",
      "[(('وكالة', 'نقلت', 'قول'), 9), (('التي', 'تنفذ', 'ها'), 6), (('التي', 'شهدت', 'ها'), 5)]\n",
      "[(('մենք', 'նայում', 'նրանց'), 3), (('ես', 'տեսել', 'քեզ'), 2), (('հակառակորդը', 'խախտել', 'ռեժիմը'), 2)]\n",
      "[(('aitak', 'du', 'izena'), 2), (('auzitegi', 'aztertuko', 'errekurtsoak'), 2), (('elkargo', 'jarri', 'zenbakia'), 2)]\n",
      "[(('гурт', 'прэзентуе', 'песню'), 6), (('мы', 'спыталі', 'мінакоў'), 5), (('яўка', 'склала', '%'), 5)]\n",
      "[(('всеки', 'има', 'право'), 6), (('той', 'иска', 'това'), 5), (('това', 'стана', 'ясно'), 4)]\n",
      "[(('джейн', 'абаба', 'медаль'), 1), (('дүүнь', 'абажа', 'велосипед'), 1), (('хүн', 'абаа', 'машина'), 1)]\n",
      "[(('que', 'fet', \"s'\"), 18), (('que', 'farà', 'es'), 15), (('que', 'celebra', 'se'), 14)]\n",
      "[(('該市', '有', '人口'), 6), (('體長', '達', '公分'), 4), (('中國', '抗', '日'), 3)]\n",
      "[(('该市', '有', '人口'), 6), (('体长', '达', '公分'), 4), (('中国', '抗', '日'), 3)]\n",
      "[(('帝', '在', '位'), 16), (('吾', '聞', '之'), 15), (('子', '言', '之'), 12)]\n",
      "[(('ϥ', 'ϫⲱ', 'ⲥ'), 37), (('ϥ', 'ϫⲟⲟ', 'ⲥ'), 26), (('ⲩ', 'ⲥⲧⲁⲩⲣⲟⲩ', 'ϥ'), 9)]\n",
      "[(('eu', 'traži', 'koje'), 4), (('albanija', 'potpisale', 'sporazum'), 3), (('albanci', 'prihvatili', 'plan'), 2)]\n",
      "[(('výbor', 'konal', 'schůze'), 8), (('které', 'mají', 'význam'), 6), (('vedoucí', 'seznámili', 'úr'), 6)]\n",
      "[(('položka', 'obsahuje', 'závazky'), 10), (('jednotka', 'uvede', 'informace'), 8), (('položka', 'obsahuje', 'pohledávky'), 8)]\n",
      "[(('vy', 'nevíte', 'nic'), 4), (('maminka', 'nenávidí', 'tě'), 3), (('větve', 'mají', 'trny'), 3)]\n",
      "[(('předseda', 'uvedl', 'to'), 17), (('předseda', 'řekl', 'to'), 15), (('ředitel', 'uvedl', 'to'), 13)]\n",
      "[(('det', 'drejer', 'sig'), 8), (('de', 'taler', 'tyrkisk'), 3), (('han', 'gjorde', 'det'), 3)]\n",
      "[(('afkorting', 'staat', 'waar'), 11), (('ik', 'gemist', 'die'), 7), (('ik', 'geweten', 'het'), 7)]\n",
      "[(('gemeenten', 'hebben', 'stad'), 6), (('gemeente', 'telt', 'inwoners'), 5), (('die', 'telt', 'leden'), 4)]\n",
      "[(('_', '_', '_'), 2516)]\n",
      "[(('you', 'have', 'questions'), 22), (('you', 'think', 'what'), 12), (('i', 'do', 'what'), 7)]\n",
      "[(('i', 'had', 'this'), 7), (('i', 'know', 'what'), 6), (('we', 'call', 'it'), 4)]\n",
      "[(('_', '_', '_'), 368)]\n",
      "[(('you', 'open', 'page'), 5), (('he', 'doing', 'what'), 4), (('he', 'found', 'himself'), 4)]\n",
      "[(('everyone', 'has', 'right'), 21), (('i', 'ask', 'you'), 4), (('you', 'distribute', 'work'), 4)]\n",
      "[(('tuul', 'liigutas', 'palli'), 7), (('ta', 'sõidutas', 'mind'), 6), (('ma', 'saan', 'aru'), 4)]\n",
      "[(('ma', 'saa', 'aru'), 7), (('ma', 'saan', 'aru'), 7), (('gravitatsioon', 'mõjutab', 'valgust'), 3)]\n",
      "[(('hann', 'segði', 'hetta'), 9), (('hann', 'sagt', 'hetta'), 8), (('tit', 'síggja', 'meg'), 8)]\n",
      "[(('minä', 'ostan', 'kirjan'), 5), (('he', 'saivat', 'tunnustusta'), 4), (('hän', 'hallinnut', 'alaa'), 4)]\n",
      "[(('joka', 'ottaa', 'perustamissopimuksen'), 14), (('meidän', 'ajatella', 'naisia'), 3), (('pohjois-korea', 'vapautti', 'amerikkalaistoimittajat'), 3)]\n",
      "[(('_', '_', '_'), 6942), (('il', '_', '_'), 153), (('elle', '_', '_'), 61)]\n",
      "[(('il', 'fait', 'partie'), 16), (('elle', 'fait', 'partie'), 7), (('il', 'comptait', 'habitants'), 7)]\n",
      "[(('personne', 'a', 'droit'), 17), (('pcb', 'accumulent', \"s'\"), 3), (('acceptant', 'crée', 'oeuvre'), 2)]\n",
      "[(('ml', 'contient', 'mg'), 4), (('flacon', 'contient', 'mg'), 3), (('médecin', 'informera', 'vous'), 3)]\n",
      "[(('je', 'vois', 'les'), 12), (('ils', 'avaient', 'peur'), 3), ((\"j'\", 'ai', 'en'), 3)]\n",
      "[(('que', 'fai', 'se'), 5), (('que', 'refire', 'se'), 5), (('regulamento', 'aproba', 'se'), 5)]\n",
      "[(('que', 'integran', 'bng'), 2), (('que', 'recollerá', 'colaboracións'), 2), (('alberto', 'recupera', 'historia'), 1)]\n",
      "[(('es', 'handelt', 'sich'), 33), (('er', 'setzte', 'sich'), 10), (('es', 'handelte', 'sich'), 7)]\n",
      "[(('es', 'handelt', 'sich'), 304), (('es', 'handele', 'sich'), 88), (('wochenschau', 'schärfen', 'blick'), 78)]\n",
      "[(('saei', 'sandida', 'mik'), 6), (('attans', 'matidedun', 'manna'), 3), (('galaubeins', 'ganasida', 'þuk'), 3)]\n",
      "[(('μιτ', 'πέτυχε', 'νίκη'), 3), (('άτομα', 'έχασαν', 'ζωή'), 2), (('αρχές', 'εξέφρασαν', 'επιθυμία'), 2)]\n",
      "[(('נעלי', 'עולות', 'ש\"ח'), 4), (('דוברים', 'הפנימו', 'קליטה'), 2), (('הוא', 'מסביר', 'זאת'), 2)]\n",
      "[(('उन्होंने', 'कहा', 'कर'), 45), (('उन्होंने', 'कहा', 'किया'), 44), (('उन्होंने', 'कहा', 'की'), 27)]\n",
      "[(('_', '_', '_'), 439)]\n",
      "[(('képviselő', 'nevezi', 'magát'), 2), (('adósság', 'taksálták', 'deficitet'), 1), (('aki', 'adta', 'magát'), 1)]\n",
      "[(('þér', 'sjá', 'mig'), 14), (('hann', 'sagði', 'þetta'), 13), (('menn', 'segja', 'það'), 13)]\n",
      "[(('ég', 'geri', 'ráð'), 15), (('ég', 'þakka', 'spurninguna'), 14), (('ég', 'hef', 'trú'), 13)]\n",
      "[(('pertumbuhan', 'mencapai', 'persen'), 5), (('perolehan', 'naik', 'persen'), 3), (('belanja', 'mencapai', 'rp'), 2)]\n",
      "[(('kota', 'memiliki', 'luas'), 6), (('penduduknya', 'berjumlah', 'jiwa'), 6), (('kota', 'memiliki', 'jumlah'), 5)]\n",
      "[(('sí', 'thug', 'leabhar'), 8), (('tú', 'fhaigheann', 'bás'), 5), (('earnáil', 'chuir', 'fostaíocht'), 4)]\n",
      "[(('individuo', 'ha', 'diritto'), 22), (('sigla', 'significa', 'cosa'), 14), (('proprietario', 'ha', 'diritto'), 11)]\n",
      "[(('individuo', 'ha', 'diritto'), 22), (('che', 'consumano', 'energia'), 6), (('shakespeare', 'scrisse', 'opere'), 3)]\n",
      "[(('governo', 'fa', 'schifo'), 5), (('goldman', 'innesca', 'crisi'), 4), (('#grillo', 'rifondare', 'nazionalsocialismo'), 3)]\n",
      "[(('chi', 'detto', 'lo'), 2), (('governo', 'cerca', 'soluzione'), 2), (('governo', 'fa', 'schifo'), 2)]\n",
      "[(('francesco', 'vinto', \"l'\"), 11), (('che', 'costituiscono', 'partecipazioni'), 6), (('chi', 'conosce', 'lo'), 5)]\n",
      "[(('_', '_', '_'), 4793)]\n",
      "[(('こと', '意味', 'こと'), 3), (('側', '起こし', '訴訟'), 2), (('年度', '目指す', '契約'), 2)]\n",
      "[(('жасаған', 'бермеді', 'бала'), 1), (('тәңірінің', 'бермеуі', 'бала'), 1), (('шолпан', 'істемеді', 'не'), 1)]\n",
      "[(('1000명', '공유하는', '동영상은'), 1), (('16명이', '입어', '부상을'), 1), (('1명이', '하면', '자살을'), 1)]\n",
      "[(('무애가', '택한', '문학을'), 2), (('사람이', '지킬', '약속을'), 2), (('언명이', '포함할', '언명을'), 2)]\n",
      "[(('dar', 'digirin', 'cihekê'), 1), (('mirov', 'bêje', 'mîrê'), 1), (('mirov', 'dike', 'behsa'), 1)]\n",
      "[(('apostolus', 'dicit', 'quod'), 14), (('agens', 'agit', 'simile'), 12), (('deus', 'produxit', 'res'), 11)]\n",
      "[(('tu', 'dedisti', 'casa'), 37), (('tu', 'firmasti', 'me'), 36), (('qui', 'tenet', 'caput'), 33)]\n",
      "[(('qui', 'habet', 'aurem'), 5), (('spiritus', 'dicat', 'quid'), 5), (('res', 'caperet', 'quid'), 2)]\n",
      "[(('qui', 'misit', 'me'), 21), (('fides', 'fecit', 'te'), 6), (('qui', 'habet', 'aures'), 6)]\n",
      "[(('genus', 'habet', 'se'), 6), (('genus', 'habere', 'se'), 3), (('quicunque', 'intendit', 'finem'), 3)]\n",
      "[(('dalībvalstis', 'veic', 'pasākumus'), 4), (('dalībvalstis', 'informē', 'komisiju'), 3), (('latvija', 'atbalsta', 'priekšlikumu'), 3)]\n",
      "[(('duomenys', 'atitinka', 'vieną'), 3), (('asmuo', 'teikia', 'pranešimą'), 2), (('asmuo', 'turi', 'teisę'), 2)]\n",
      "[(('aristofanas', 'nemėgsta', 'jo'), 1), (('asmenys', 'apmėto', 'eitynes'), 1), (('aukos', 'vertos', 'pagarbos'), 1)]\n",
      "[(('häi', 'osti', 'mašinan'), 1), (('ken', 'kirjutti', 'sen'), 1), (('mary', 'voitti', 'bronzumedalin'), 1)]\n",
      "[(('frott', 'jieħu', 'lewn'), 2), (('għadd', 'għandu', 'proprjetà'), 2), (('jien', 'għandix', 'oġġezzjoni'), 2)]\n",
      "[(('त्याने', 'फोडला', 'लाडू'), 2), (('_', 'मिळाले', 'रुपये'), 1), (('_', 'लावली', 'कलमे'), 1)]\n",
      "[(('i', 'tell', 'you'), 44), (('i', 'tell', 'am'), 23), (('you', 'put', 'am'), 23)]\n",
      "[(('mun', 'ožžon', 'veahki'), 4), (('mun', 'oaidnán', 'vielljan'), 3), (('boanda', 'goddá', 'vuovssá'), 2)]\n",
      "[(('jeg', 'føler', 'meg'), 7), (('jeg', 'har', 'lyst'), 7), (('jeg', 'har', 'spørsmål'), 6)]\n",
      "[(('politiet', 'fekk', 'melding'), 8), (('dei', 'gjer', 'det'), 5), (('dei', 'kjenner', 'seg'), 5)]\n",
      "[(('eg', 'trur', 'det'), 14), (('dei', 'gjorde', 'det'), 9), (('dei', 'sa', 'det'), 9)]\n",
      "[(('азъ', 'изгонѭ', 'бѣсꙑ'), 4), (('азъ', 'дамь', 'ѭже'), 3), (('вꙑ', 'вѣсте', 'егоже'), 3)]\n",
      "[(('денщик', 'подал', 'грамотку'), 2), (('онъ', 'пришлетъ', 'кого'), 2), (('послы', 'целовали', 'крестъ'), 2)]\n",
      "[(('кнѧзь', 'посла', 'сн҃а'), 5), (('ѥму', 'платити', 'гр҃и'), 4), (('влд҃ка', 'ст҃и', 'ю'), 3)]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(('il', 'dit', 'ce'), 10), (('je', 'pri', 'vos'), 8), (('il', 'voit', 'ce'), 7)]\n",
      "[(('ما', 'کرد', 'شما'), 4), (('گمرک', 'می\\u200cگرفت', 'عوارض'), 4), (('او', 'کرد', 'تمام'), 3)]\n",
      "[(('رستوران', 'دارد', 'ارزش'), 3), (('یکی', 'می\\u200cکند', 'توجه'), 3), (('احمد', 'برمی\\u200cدارد', 'سر'), 2)]\n",
      "[(('eseldziaki', 'kantowały', 'kumpli'), 4), (('aleksander', 'dokonał', 'otwarcia'), 3), (('kaczka', 'ugryzie', 'nikogo'), 3)]\n",
      "[(('człowiek', 'wykonuje', 'skok'), 15), (('chłopak', 'wykonuje', 'skok'), 8), (('mężczyzna', 'robi', 'zdjęcie'), 5)]\n",
      "[(('que', 'dizem', 'respeito'), 3), (('bc', 'comprar', 'dólares'), 2), (('eu', 'faço', 'campanha'), 2)]\n",
      "[(('diagrama', 'representa', 'localidades'), 4), (('que', 'faz', 'parte'), 3), (('que', 'levou', 'o'), 3)]\n",
      "[(('ce', 'trimes', 'm-'), 15), (('ce', 'face', 'diiată'), 10), (('tu', 'scoate', 'mă'), 10)]\n",
      "[(('statele', 'iau', 'măsurile'), 10), (('comisia', 'acordat', 'autorizație'), 7), (('condițiile', 'cuprind', 'clauze'), 5)]\n",
      "[(('care', 'inclus', 'pacienți'), 8), (('care', 'consumau', 'statine'), 5), (('care', 'purtat', 'scutece'), 4)]\n",
      "[(('население', 'составляет', 'человек'), 10), (('население', 'составляло', 'человек'), 8), (('длина', 'составляет', 'км'), 6)]\n",
      "[(('мы', 'имеем', 'дело'), 6), (('мы', 'имеем', 'что'), 4), (('мы', 'сделаем', 'все'), 4)]\n",
      "[(('это', 'значит', 'что'), 4), (('все', 'устраивает', 'меня'), 3), (('это', 'беспокоит', 'меня'), 3)]\n",
      "[(('devāḥ', 'ajayan', 'svargam'), 4), (('aham', 'avocam', 'viṣam'), 3), (('maitrāvaruṇaḥ', 'pratipadyate', 'upapraiṣam'), 3)]\n",
      "[(('e', 'chuir', 'i'), 12), (('esan', 'chuir', 'i'), 9), (('e', 'chuir', 'cnot'), 4)]\n",
      "[(('albanci', 'prihvatili', 'plan'), 2), (('albanija', 'potpisale', 'sporazum'), 2), (('ebrd', 'održala', 'skupštinu'), 2)]\n",
      "[(('vláda', 'venovať', 'pozornosť'), 6), (('vláda', 'vytvorí', 'podmienky'), 4), (('to', 'prekvapilo', 'ma'), 3)]\n",
      "[(('to', 'pomeni', 'kaj'), 3), (('delavec', 'odpoveduje', 'pogodbo'), 2), (('predstojnik', 'izvršuje', 'pravice'), 2)]\n",
      "[(('jaz', 'vem', 'kaj'), 2), (('kar', 'tiče', 'tega'), 2), (('vi', 'pravite', 'kaj'), 2)]\n",
      "[(('personas', 'resultaron', 'heridas'), 4), (('que', 'supone', 'incremento'), 4), (('efe', 'tenido', 'acceso'), 3)]\n",
      "[(('hombres', 'tenían', 'ingresos'), 18), (('municipio', 'tiene', 'superficie'), 18), (('condado', 'tiene', 'área'), 3)]\n",
      "[(('han', 'befann', 'sig'), 6), (('du', 'flyttar', 'linje'), 4), (('han', 'beslöt', 'sig'), 4)]\n",
      "[(('man', 'vänder', 'sig'), 14), (('det', 'rör', 'sig'), 5), (('man', 'söker', 'arbete'), 3)]\n",
      "[(('groda', 'följas-åt*med', 'pojke'), 1), (('pro1', 'beställa', 'tolk'), 1), (('pro1', 'hand(q)+hantera@p', 'tolk'), 1)]\n",
      "[(('அவர்', 'பார்வையிட்டார்', 'முகாமை'), 2), (('சட்டம்', 'பறிக்கும்', 'உரிமைகளைப்'), 2), (('அதிகாரி', 'தெரிவித்த்', 'கைது'), 1)]\n",
      "[(('ఆమె', 'లేదు', 'సంగతి'), 3), (('రాము', 'ఇచ్చింది', 'పుస్తకం'), 3), (('కోపం', 'వచ్చింది', 'కమలకు'), 2)]\n",
      "[(('komisyonun', 'tamamlaması', 'çalışmalarını'), 2), (('çocuklar', 'oldu', 'kardeş'), 2), (('\"acelem', 'var', 'sorularını'), 1)]\n",
      "[(('ben', 'görmemişçesine', 'onu'), 2), (('beşir', 'etti', 'olanları'), 2), (('hakim', 'etti', 'celseyi'), 2)]\n",
      "[(('yüzde', 'okuyor', 'arkasını'), 3), (('yüzde', 'okuyor', 'yüzlerini'), 3), (('yüzde', 'okuyor', 'önyüzünü'), 3)]\n",
      "[(('beyefendi', 'sürüştürüyor', 'allık'), 3), (('cambazları', 'gösterirler', 'hünerlerini'), 3), (('misketi', 'hırpalamış', 'bacağını'), 3)]\n",
      "[(('araçları', 'ediyor', 'siparişini'), 7), (('gelir', 'geldi', \"doları'na\"), 4), (('hacim', 'erişti', 'hisseye'), 3)]\n",
      "[(('ben', 'yaptirmadim', 'rezervasyonu'), 18), (('ben', 'yapmadim', 'rezervasyonu'), 8), (('arkadaşim', 'yaptirdi', 'rezervasyonu'), 4)]\n",
      "[(('ich', 'weiß', 'es'), 3), (('das', 'macht', 'spaß'), 2), (('ich', 'gemacht', 'sport'), 2)]\n",
      "[(('вона', 'каже', 'йому'), 6), (('ключ', 'знадобиться', 'вам'), 3), (('що', 'дозволило', 'йому'), 3)]\n",
      "[(('heinrich', 'podćisnje', 'kmjenaj'), 1), (('historikarjo', 'mjenuja', 'dobu'), 1), (('kmjeny', 'wobsydla', 'krajinu'), 1)]\n",
      "[(('انہوں', 'کہا', 'کیا'), 20), (('انہوں', 'کہا', 'کی'), 17), (('انہوں', 'کہا', 'ہے'), 12)]\n",
      "[(('ئۇ', 'يېمەيتتى', 'نەرسە'), 3), (('قىز', 'يېدى', 'يوپۇرمىقى'), 3), (('كىشىلەر', 'دەيدۇ', 'مېنى'), 3)]\n",
      "[(('anh', 'ở', 'đây'), 2), (('bà con', 'tạo', 'điều kiện'), 2), (('em', 'ở', 'đâu'), 2)]\n",
      "[(('a', 'gafodd', 'mlynedd'), 1), (('a', 'goncrodd', 'everest'), 1), (('athrawes', 'enillodd', 'ysgoloriaeth'), 1)]\n",
      "[(('մենք', 'ունինք', 'փիլիսոփայութիւնը'), 2), (('pensionnaire', 'ընէին', 'պտոյտը'), 1), (('ակնարկը', 'պատահեցաւ', 'լուսանկարին'), 1)]\n",
      "[(('nit', 'am', 'sañ-sañ'), 13), (('nga', 'xam', 'yi'), 5), (('mu', 'am', 'lu'), 4)]\n"
     ]
    }
   ],
   "source": [
    "nbest=3\n",
    "triples=[];\n",
    "for file in files:\n",
    "    sentences = read_sentences(file)\n",
    "    dictt=convert_to_dict(split_rows(sentences, column_names_u))\n",
    "    temp=extract_pairs_and_triples(dictt, nbest)\n",
    "    triples.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('det', 'drejer', 'sig'), 8),\n",
       " (('de', 'taler', 'tyrkisk'), 3),\n",
       " (('han', 'gjorde', 'det'), 3)]"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "triples[20][1]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In your report, you will include the `nbest` most frequent pairs and triples you obtained in **three languages**. You may choose the ones you want.\n",
    "\n",
    "For the checking script, you will extract `nbest` triples in French, Russian, and English. You will rank these triples by frequency, and then by alphabetical order of the triple using `sorted()`. You will use the French GSD corpus, the Russian SynTagRus corpus, and the English EWT corpus. You will store these triples in the following variables:\n",
    "`freq_triples_fr`, `freq_triples_ru`, `freq_triples_en`. Each variable will contain a list of tuples: `(subject, verb, object), freq)`\n",
    "\n",
    "Here is what you should find:\n",
    "\n",
    "French\n",
    "```\n",
    "freq_triples_fr = [(('il', 'fait', 'partie'), 16),\n",
    "\n",
    " ...]\n",
    " ````\n",
    "\n",
    "And the frequencies:\n",
    "```\n",
    "16\n",
    "7\n",
    "7\n",
    "```\n",
    "\n",
    "Russian:\n",
    "```\n",
    "freq_triples_ru = [(('мы', 'имеем', 'дело'), 6),\n",
    "\n",
    " ...]\n",
    " ````\n",
    "\n",
    "And the frequencies:\n",
    "```\n",
    "6\n",
    "4\n",
    "4\n",
    "```\n",
    "\n",
    "English:\n",
    "```\n",
    "freq_triples_en = [(('you', 'have', 'questions'), 22),\n",
    "\n",
    " ...]\n",
    " ````\n",
    "\n",
    "And the frequencies:\n",
    "```\n",
    "22\n",
    "12\n",
    "7\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = [path_fr, path_ru, path_en]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### French"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "nbest=3\n",
    "sentences = read_sentences(path_fr)\n",
    "formatted_corpus_dict=convert_to_dict(split_rows(sentences, column_names_u))\n",
    "#triples=extract_triples(formatted_corpus_dict)\n",
    "\n",
    "#sorted_triples= sorted (triples, key=lambda x: (-triples[x],x))\n",
    "#[(triple, triples[triple]) for triple in sorted_triples][:nbest]   \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(('il', 'fait', 'partie'), 16), (('elle', 'fait', 'partie'), 7), (('il', 'comptait', 'habitants'), 7)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(('il', 'fait', 'partie'), 16),\n",
       " (('elle', 'fait', 'partie'), 7),\n",
       " (('il', 'comptait', 'habitants'), 7)]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq_pairs_fr, freq_triples_fr = extract_pairs_and_triples(formatted_corpus_dict, nbest)\n",
    "freq_triples_fr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Russian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "nbest=3\n",
    "sentences = read_sentences(path_ru)\n",
    "formatted_corpus_dict=convert_to_dict(split_rows(sentences, column_names_u))\n",
    "#triples=extract_triples(formatted_corpus_dict)\n",
    "#sorted_triples= sorted (triples, key=lambda x: (-triples[x],x))\n",
    "#[(triple, triples[triple]) for triple in sorted_triples][:nbest]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(('мы', 'имеем', 'дело'), 6), (('мы', 'имеем', 'что'), 4), (('мы', 'сделаем', 'все'), 4)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(('мы', 'имеем', 'дело'), 6),\n",
       " (('мы', 'имеем', 'что'), 4),\n",
       " (('мы', 'сделаем', 'все'), 4)]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq_pairs_ru, freq_triples_ru = extract_pairs_and_triples(formatted_corpus_dict, nbest)\n",
    "freq_triples_ru"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "nbest=3\n",
    "sentences = read_sentences(path_en)\n",
    "formatted_corpus_dict=convert_to_dict(split_rows(sentences, column_names_u))\n",
    "#triples=extract_triples(formatted_corpus_dict)\n",
    "#sorted_triples= sorted (triples, key=lambda x: (-triples[x],x))\n",
    "#[(triple, triples[triple]) for triple in sorted_triples][:nbest]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(('you', 'have', 'questions'), 22), (('you', 'think', 'what'), 12), (('i', 'do', 'what'), 7)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(('you', 'have', 'questions'), 22),\n",
       " (('you', 'think', 'what'), 12),\n",
       " (('i', 'do', 'what'), 7)]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq_pairs_en, freq_triples_en = extract_pairs_and_triples(formatted_corpus_dict, 3)\n",
    "freq_triples_en"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resolving the entities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will now extract the relations involving named entities, that is where both the subject and the object are proper nouns. These proper nouns have a specific part of speech in the `UPOS` column.\n",
    "\n",
    "Write an `extract_entity_triples(formatted_corpus_dict)` that will process the corpus and return a list of `(subject, verb, object)` triples. You will leave the case as it is in the `FORM` column, for instance _United States_ and not _united states_.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_entity_triples(formatted_corpus_dict):\n",
    "    triples_sv={}\n",
    "    for sen in formatted_corpus_dict:\n",
    "        for row in sen.keys():\n",
    "            if sen[row]['DEPREL'].startswith('nsubj') and sen[row]['UPOS'].startswith('PROPN'):\n",
    "                HID=sen[row]['HEAD']\n",
    "                for rows in sen.keys():\n",
    "                    if sen[rows]['DEPREL'].startswith('obj') and sen[rows]['UPOS'].startswith('PROPN'):\n",
    "                        HID2=sen[rows]['HEAD']\n",
    "                        if HID==HID2:\n",
    "                            tup=(sen[row]['FORM'],sen[HID]['FORM'],sen[rows]['FORM'])\n",
    "                            if tup in triples_sv.keys():\n",
    "                                triples_sv[tup] +=1\n",
    "                            else:\n",
    "                                triples_sv[tup] =1\n",
    "    return triples_sv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will run the `extract_entity_triples()` function one the English EWT corpus. You will store the list in the `entity_relation_en` variable and you will sort it with `sorted()`. You will keep the **five** first triples. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "nbest = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The two first triples are:\n",
    "```\n",
    "[('Baba', 'remember', 'George'),\n",
    " ('Beschta', 'told', 'Planet'),\n",
    "...]\n",
    " ```\n",
    "Note that this time, we keep the original case and the triples are in the alphabetical order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "triples=extract_entity_triples(formatted_corpus_dict)\n",
    "entity_relation_en= sorted (triples, key=lambda x: (-triples[x],x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Baba', 'remember', 'George'),\n",
       " ('Beschta', 'told', 'Planet'),\n",
       " ('Boi', 'beat', 'Lopez'),\n",
       " ('Bush', 'mentioned', 'Arabia'),\n",
       " ('Bush', 'mentioned', 'Osama')]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entity_relation_en = sorted(entity_relation_en)[:nbest]\n",
    "entity_relation_en"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extracting only the headword of the subject and object if often incomplete and uninformative. You can extract all the chunk instead. \n",
    "\n",
    "As an exercise, you will extract manually from the corpus the complete proper nouns in this triple `('Baba', 'remember', 'George')`. You will first consider `Baba` and `George`. You will then look at the verb `remember`.\n",
    "\n",
    "**In your report**, you will give the complete value of the two proper nouns as well as the verb: `(Complete value of the subject, Complete value of the verb, Complete value of the object)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional exercise: Automatic Extraction of the chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an optional exercise, you can implement an automatic baseline technique and extract adjacent proper nouns. You may also want to apply the chunker of the 4th assignment to the corpus to do this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Optional exercise: Mapping the entities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As in the chunker assignment, you may also want to complement your assignment with a entity solver that will link the entities to wikidata."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the article: _PRISMATIC: Inducing Knowledge from a Large Scale Lexicalized Relation Resource_ by Fan and al. (2010) [<a href=\"http://www.aclweb.org/anthology/W/W10/W10-0915.pdf\">pdf</a>] and write in a few sentences how it relates to your work in this assignment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you have written all the code and run all the cells, fill in your ID and as well as the name of the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "STIL_ID = [\"mo0708ho-s\", \"fa0188ni-s\"] \n",
    "CURRENT_NOTEBOOK_PATH = os.path.join(os.getcwd(), \n",
    "                                     \"5-triples_solution.ipynb\") # Write the name of your notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The submission code will send your answer. It consists of the pairs and triples in four languages, as well as the triples with named entities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"freq_triples_sv\": [[[\"man\", \"v\\\\u00e4nder\", \"sig\"], 14], [[\"det\", \"r\\\\u00f6r\", \"sig\"], 5], [[\"man\", \"s\\\\u00f6ker\", \"arbete\"], 3]], \"freq_triples_fr\": [[[\"il\", \"fait\", \"partie\"], 16], [[\"elle\", \"fait\", \"partie\"], 7], [[\"il\", \"comptait\", \"habitants\"], 7]], \"freq_triples_ru\": [[[\"\\\\u043c\\\\u044b\", \"\\\\u0438\\\\u043c\\\\u0435\\\\u0435\\\\u043c\", \"\\\\u0434\\\\u0435\\\\u043b\\\\u043e\"], 6], [[\"\\\\u043c\\\\u044b\", \"\\\\u0438\\\\u043c\\\\u0435\\\\u0435\\\\u043c\", \"\\\\u0447\\\\u0442\\\\u043e\"], 4], [[\"\\\\u043c\\\\u044b\", \"\\\\u0441\\\\u0434\\\\u0435\\\\u043b\\\\u0430\\\\u0435\\\\u043c\", \"\\\\u0432\\\\u0441\\\\u0435\"], 4]], \"freq_triples_en\": [[[\"you\", \"have\", \"questions\"], 22], [[\"you\", \"think\", \"what\"], 12], [[\"i\", \"do\", \"what\"], 7]], \"entity_relation_en\": [[\"Baba\", \"remember\", \"George\"], [\"Beschta\", \"told\", \"Planet\"], [\"Boi\", \"beat\", \"Lopez\"], [\"Bush\", \"mentioned\", \"Arabia\"], [\"Bush\", \"mentioned\", \"Osama\"]]}'"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "ANSWER = json.dumps({'freq_triples_sv': freq_triples_sv,\n",
    "                     'freq_triples_fr': freq_triples_fr,\n",
    "                     'freq_triples_ru': freq_triples_ru,\n",
    "                     'freq_triples_en': freq_triples_en,\n",
    "                     'entity_relation_en': entity_relation_en\n",
    "                    })\n",
    "ANSWER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the moment of truth:\n",
    "1. Save your notebook and\n",
    "2. Run the cells below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "SUBMISSION_NOTEBOOK_PATH = CURRENT_NOTEBOOK_PATH + \".submission.bz2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bz2\n",
    "ASSIGNMENT = 5\n",
    "API_KEY = \"f581ba347babfea0b8f2c74a3a6776a7\"\n",
    "\n",
    "# Copy and compress current notebook\n",
    "with bz2.open(SUBMISSION_NOTEBOOK_PATH, mode=\"wb\") as fout:\n",
    "    with open(CURRENT_NOTEBOOK_PATH, \"rb\") as fin:\n",
    "        fout.write(fin.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'msg': None,\n",
       " 'status': 'correct',\n",
       " 'signature': '77bf9458d21cccc2385e7277546f0d9fbe1f2f276b231843bd3d3258caa8b56629e7c84f7a9db913d5ad73e1d9b6a3450af450bce31ee8d63768dd82944ec4a1',\n",
       " 'submission_id': '10e9688d-803c-493e-94e9-e390c8ab6de9'}"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "res = requests.post(\"https://vilde.cs.lth.se/edan20checker/submit\", \n",
    "                    files={\"notebook_file\": open(SUBMISSION_NOTEBOOK_PATH, \"rb\")}, \n",
    "                    data={\n",
    "                        \"stil_id\": STIL_ID,\n",
    "                        \"assignment\": ASSIGNMENT,\n",
    "                        \"answer\": ANSWER,\n",
    "                        \"api_key\": API_KEY,\n",
    "                    },\n",
    "               verify=True)\n",
    "\n",
    "# from IPython.display import display, JSON\n",
    "res.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
